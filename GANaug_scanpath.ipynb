{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPm0wuIkPwTB0qMpxSzl9zI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mohan0332/Capstone/blob/main/GANaug_scanpath.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "iKLSeLsIZdIK"
      },
      "outputs": [],
      "source": [
        "from keras.datasets import mnist\n",
        "from keras.layers import Input, Dense, Reshape, Flatten\n",
        "from keras.layers import LeakyReLU\n",
        "from keras.layers import Activation\n",
        "\n",
        "from keras.models import Sequential, Model\n",
        "from keras.optimizers import Adam\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import sys\n",
        "\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "img_rows = 256\n",
        "img_cols = 192\n",
        "channels = 4\n",
        "img_shape = (img_rows, img_cols, channels)\n",
        "\n",
        "z_dim = 100"
      ],
      "metadata": {
        "id": "98K3PGVTaFJw"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "40PLYb5maWIv",
        "outputId": "93b14e33-4d5f-46c2-9de1-3607454209a9"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "GENERATOR NETWORK"
      ],
      "metadata": {
        "id": "jqbzQtjsawwP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generator(img_shape, z_dim):\n",
        "    \n",
        "    model = Sequential()\n",
        "    \n",
        "    # Hidden layer\n",
        "    model.add(Dense(128, input_dim=z_dim))\n",
        "\n",
        "    # Leaky ReLU\n",
        "    model.add(LeakyReLU(alpha=0.01))\n",
        "\n",
        "    # Output layer with tanh activation\n",
        "    model.add(Dense(256*192*3, activation='tanh'))\n",
        "    model.add(Reshape(img_shape))\n",
        "\n",
        "    z = Input(shape=(z_dim,))\n",
        "    img = model(z)\n",
        "\n",
        "    return Model(z, img)"
      ],
      "metadata": {
        "id": "eq4ke7FRa1EQ"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Discriminator Network"
      ],
      "metadata": {
        "id": "qk4iry42bNwH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def discriminator(img_shape):\n",
        "    \n",
        "    model = Sequential()\n",
        "\n",
        "    model.add(Flatten(input_shape=img_shape))\n",
        "\n",
        "    # Hidden layer\n",
        "    model.add(Dense(128))\n",
        "\n",
        "    # Leaky ReLU\n",
        "    model.add(LeakyReLU(alpha=0.01))\n",
        "    # Output layer with sigmoid activation\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "    img = Input(shape=img_shape)\n",
        "    prediction = model(img)\n",
        "\n",
        "    return Model(img, prediction)"
      ],
      "metadata": {
        "id": "AhdrmP9hbOKf"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "discriminator = discriminator(img_shape)\n",
        "discriminator.compile(loss='binary_crossentropy', \n",
        "                      optimizer=Adam(), metrics=['accuracy'])\n",
        "\n",
        "# Build the Generator\n",
        "generator = generator(img_shape, z_dim)\n",
        "\n",
        "# Generated image to be used as input\n",
        "z = Input(shape=(100,))\n",
        "img = generator(z)\n",
        "\n",
        "# Keep Discriminator’s parameters constant during Generator training\n",
        "discriminator.trainable = False\n",
        "\n",
        "# The Discriminator’s prediction\n",
        "prediction = discriminator(img)\n",
        "\n",
        "# Combined GAN model to train the Generator\n",
        "combined = Model(z, prediction)\n",
        "combined.compile(loss='binary_crossentropy', optimizer=Adam())"
      ],
      "metadata": {
        "id": "TUbpVNsAbYYo"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loading the dataset and converting it to a numpy array"
      ],
      "metadata": {
        "id": "elFByp_hbwBP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from PIL import Image\n",
        "data = []\n",
        "path = '/content/drive/MyDrive/Capstone dataset/Images-out/train/TCImages'\n",
        "images = os.listdir(path)\n",
        "for a in images:\n",
        "        try:\n",
        "            image = Image.open(path + '/'+ a).convert('RGB')\n",
        "            image = image.resize((192,256))\n",
        "            image = np.array(image)\n",
        "            #sim = Image.fromarray(image)\n",
        "            data.append(image)\n",
        "        except:\n",
        "            print(\"Error loading image\")"
      ],
      "metadata": {
        "id": "9tfIDvsdiERJ"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Converting lists into numpy arrays\n",
        "data = np.array(data)"
      ],
      "metadata": {
        "id": "GBLvBZtXi088"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = data\n",
        "X_train.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yDV9ohNPi4ni",
        "outputId": "afde3e64-a9ba-46a4-ec80-2242c8c3d558"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(229, 256, 192, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Rescale [0, 255] grayscale pixel values to [-1, 1]\n",
        "X_train = X_train / 127.5 - 1.0"
      ],
      "metadata": {
        "id": "bxxbFKJWjwxQ"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "GAN Training function"
      ],
      "metadata": {
        "id": "A8gq2YhKnock"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "losses = []\n",
        "accuracies = []\n",
        "\n",
        "def train(iterations, batch_size, sample_interval):\n",
        "  \n",
        "    # Labels for real and fake examples\n",
        "    real = np.ones((batch_size, 1))\n",
        "    fake = np.zeros((batch_size, 1))\n",
        "\n",
        "    for iteration in range(iterations):\n",
        "        \n",
        "        # -------------------------\n",
        "        #  Train the Discriminator\n",
        "        # -------------------------\n",
        "\n",
        "        # Select a random batch of real images\n",
        "        idx = np.random.randint(0, X_train.shape[0], batch_size)\n",
        "        imgs = X_train[idx]\n",
        "\n",
        "        # Generate a batch of fake images\n",
        "        z = np.random.normal(0, 1, (batch_size, 100))\n",
        "        gen_imgs = generator.predict(z)\n",
        "\n",
        "        # Discriminator loss\n",
        "        d_loss_real = discriminator.train_on_batch(imgs, real)\n",
        "        d_loss_fake = discriminator.train_on_batch(gen_imgs, fake)\n",
        "        d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
        "        \n",
        "        z = np.random.normal(0, 1, (batch_size, 100))\n",
        "        gen_imgs = generator.predict(z)\n",
        "\n",
        "        # Generator loss\n",
        "        g_loss = combined.train_on_batch(z, real)\n",
        "\n",
        "        if iteration % sample_interval == 0:\n",
        "            \n",
        "            # Output training progress\n",
        "            print (\"%d [D loss: %f, acc.: %.2f%%] [G loss: %f]\" % \n",
        "                         (iteration, d_loss[0], 100*d_loss[1], g_loss))\n",
        "            \n",
        "            # Save losses and accuracies so they can be plotted after training\n",
        "            losses.append((d_loss[0], g_loss))\n",
        "            accuracies.append(100*d_loss[1])\n",
        "\n",
        "            # Output generated image samples \n",
        "            sample_images(iteration)"
      ],
      "metadata": {
        "id": "uhz3Fd25nTjj"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sample_images(iteration, image_grid_rows=4, image_grid_columns=4):\n",
        "\n",
        "    # Sample random noise\n",
        "    z = np.random.normal(0, 1, \n",
        "              (image_grid_rows * image_grid_columns, z_dim))\n",
        "\n",
        "    # Generate images from random noise \n",
        "    gen_imgs = generator.predict(z)\n",
        "\n",
        "    # Rescale images to 0-1\n",
        "    gen_imgs = 0.5 * gen_imgs + 0.5\n",
        "\n",
        "    \n",
        "    # Set image grid\n",
        "    fig, axs = plt.subplots(image_grid_rows, image_grid_columns, \n",
        "                                    figsize=(4,4), sharey=True, sharex=True)\n",
        "    \n",
        "    cnt = 0\n",
        "    for i in range(image_grid_rows):\n",
        "        for j in range(image_grid_columns):\n",
        "            # Output image grid\n",
        "            axs[i,j].imshow(gen_imgs[cnt, :,:,0], cmap='gray')\n",
        "            axs[i,j].axis('off')\n",
        "            cnt += 1"
      ],
      "metadata": {
        "id": "BIt2V6dtoyTD"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings; warnings.simplefilter('ignore')"
      ],
      "metadata": {
        "id": "wo9N627co0pt"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "iterations = 10000\n",
        "batch_size = 32\n",
        "sample_interval = 1000\n",
        "\n",
        "# Train the GAN for the specified number of iterations\n",
        "train(iterations, batch_size, sample_interval)"
      ],
      "metadata": {
        "id": "yfzLIvQzq5bG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}